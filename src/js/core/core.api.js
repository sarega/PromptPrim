// ===============================================
// FILE: src/js/core/core.api.js 
// DESCRIPTION: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Model ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô Gemma 3 27B
// ===============================================

import { stateManager } from './core.state.js';

// --- Helper sub-functions (not exported, private to this module) ---
async function fetchOpenRouterModels(apiKey) {
    const response = await fetch('https://openrouter.ai/api/v1/models', { headers: { 'Authorization': `Bearer ${apiKey}` } });
    if (!response.ok) throw new Error('Could not fetch models from OpenRouter');
    const data = await response.json();
    return data.data.map(m => ({ id: m.id, name: m.name || m.id, provider: 'openrouter' }));
}

async function fetchOllamaModels(baseUrl) {
    try {
        const response = await fetch(`${baseUrl}/api/tags`);
        if (!response.ok) throw new Error(`Could not connect to Ollama (HTTP ${response.status})`);
        const data = await response.json();
        return data.models.map(m => ({ id: m.name, name: m.name, provider: 'ollama'}));
    } catch (error) {
        if (error instanceof TypeError) throw new Error('Could not connect to Ollama. Check URL, server status, and CORS settings.');
        throw error;
    }
}

/**
 * [REWRITTEN] A robust fetch helper that supports both an external AbortSignal
 * (for the stop button) and an internal timeout.
 * @param {string} resource - The URL to fetch.
 * @param {object} options - The options for the fetch request, may include a signal.
 * @param {number} [timeout=120000] - The timeout in milliseconds.
 * @returns {Promise<Response>}
 */
async function fetchWithTimeout(resource, options = {}, timeout = 120000) {
    // 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á AbortController ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Timeout ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞
    const timeoutController = new AbortController();
    const timeoutId = setTimeout(() => timeoutController.abort(), timeout);

    // 2. [‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î] ‡∏£‡∏ß‡∏° signal ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å (‡∏õ‡∏∏‡πà‡∏° Stop) ‡∏Å‡∏±‡∏ö signal ‡∏Ç‡∏≠‡∏á Timeout
    // ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ AbortSignal.any() ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠ signal ‡πÉ‡∏î signal ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å
    // ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° Stop ‡∏´‡∏£‡∏∑‡∏≠ Timeout ‡∏Å‡πá‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏´‡∏¢‡∏∏‡∏î fetch ‡πÑ‡∏î‡πâ
    const abortSignal = AbortSignal.any([
        options.signal,
        timeoutController.signal
    ].filter(Boolean)); // .filter(Boolean) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏≠‡∏á signal ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô null ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ

    try {
        // 3. ‡∏™‡πà‡∏á AbortSignal ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô fetch request
        const response = await fetch(resource, {
            ...options,
            signal: abortSignal,
        });
        return response;
    } finally {
        // 4. ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤ fetch ‡∏à‡∏∞‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡πÉ‡∏´‡πâ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå Timeout ‡πÄ‡∏™‡∏°‡∏≠
        clearTimeout(timeoutId);
    }
}


// --- Main Exported Functions ---

export async function loadAllProviderModels() {
    stateManager.bus.publish('status:update', { message: 'Loading models...', state: 'loading' });
    
    // [FIX 1] ‡∏™‡∏£‡πâ‡∏≤‡∏á deep copy ‡∏Ç‡∏≠‡∏á project state ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô side effect
    const project = JSON.parse(JSON.stringify(stateManager.getProject()));

    if (!project || !project.globalSettings) {
        stateManager.bus.publish('status:update', { message: 'Project not loaded', state: 'error' });
        return;
    }

    const apiKey = project.globalSettings.apiKey?.trim() || '';
    const baseUrl = project.globalSettings.ollamaBaseUrl?.trim() || '';
    
    let allModels = [];
    try {
        const fetchPromises = [];
        if (apiKey) fetchPromises.push(fetchOpenRouterModels(apiKey).catch(e => { console.error("OpenRouter fetch failed:", e); return []; }));
        if (baseUrl) fetchPromises.push(fetchOllamaModels(baseUrl).catch(e => { console.error("Ollama fetch failed:", e); return []; }));

        const results = await Promise.all(fetchPromises);
        allModels = results.flat();
    } catch (error) {
        stateManager.bus.publish('status:update', { message: `Error loading models: ${error.message}`, state: 'error' });
        return;
    }

    // --- [FIX 2] ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Logic ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Default Model ---
    let projectWasChanged = false;
    if (allModels.length > 0) {
        // ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Model ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        const preferredDefaults = [
            'google/gemma-3-27b-it',
            'google/gemma-2-9b-it',
            'openai/gpt-4o-mini',
            'mistralai/mistral-7b-instruct'
        ];
        
        // ‡∏´‡∏≤ Model ‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ `preferredDefaults` ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏°‡∏µ
        const availableDefaultModel = preferredDefaults.find(pdm => allModels.some(am => am.id === pdm));

        if (availableDefaultModel) {
            // 1. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï System Utility Agent
            const systemAgent = project.globalSettings.systemUtilityAgent;
            if (!systemAgent.model) { // ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ model ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
                systemAgent.model = availableDefaultModel;
                projectWasChanged = true;
                console.log(`System Utility Agent model set to: ${availableDefaultModel}`);
            }

            // 2. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Default Agent
            const defaultAgent = project.agentPresets['Default Agent'];
            if (defaultAgent && !defaultAgent.model) {
                defaultAgent.model = availableDefaultModel;
                projectWasChanged = true;
                console.log(`'Default Agent' model set to: ${availableDefaultModel}`);
            }
        }
    }
    
    // --- [FIX 3] ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ State Update ‡πÅ‡∏•‡∏∞ UI Re-render ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô ---
    
    // 1. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ Model ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô State (‡∏à‡∏∞ trigger ‡πÉ‡∏´‡πâ dropdown ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•)
    stateManager.setAllModels(allModels);
    
    // 2. ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ Default Model ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï project state ‡∏ó‡∏±‡πâ‡∏á‡∏Å‡πâ‡∏≠‡∏ô
    if (projectWasChanged) {
        stateManager.setProject(project); // ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï state ‡πÉ‡∏ô memory
        await stateManager.updateAndPersistState(); // ‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏ã‡∏ü‡∏•‡∏á DB ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ isDirty
    }
    
    // 3. ‡πÅ‡∏à‡πâ‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    const statusMessage = allModels.length > 0 ? `Loaded ${allModels.length} models.` : 'No models found. Check API Settings.';
    stateManager.bus.publish('status:update', { message: statusMessage, state: 'connected' });
}

export async function callLLM(agent, messages) {
    const allModels = stateManager.getState().allProviderModels;
    const modelData = allModels.find(m => m.id === agent.model);
    if (!modelData) throw new Error("Model data not found for the agent.");

    const project = stateManager.getProject();
    const provider = modelData.provider;
    
    // ‡∏™‡∏£‡πâ‡∏≤‡∏á Object ‡∏Ç‡∏≠‡∏á body ‡πÅ‡∏•‡∏∞ params ‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
    const body = { 
        model: agent.model, 
        messages: messages, 
        stream: false 
    };
    
    const params = {
        temperature: parseFloat(agent.temperature), 
        top_p: parseFloat(agent.topP),
        top_k: parseInt(agent.topK, 10), 
        presence_penalty: parseFloat(agent.presence_penalty),
        frequency_penalty: parseFloat(agent.frequency_penalty), 
        max_tokens: parseInt(agent.max_tokens, 10),
        seed: parseInt(agent.seed, 10),
    };

    if (agent.stop_sequences) {
        params.stop = agent.stop_sequences.split(',').map(s => s.trim());
    }

    let url, headers;

    if (provider === 'openrouter') {
        url = 'https://openrouter.ai/api/v1/chat/completions';
        headers = { 'Authorization': `Bearer ${project.globalSettings.apiKey}`, 'Content-Type': 'application/json' };
        
        // [FIX] ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Web Search
        body.tools = [
            { "type": "Google Search" }
        ];

        Object.assign(body, params);

    } else { // ollama
        url = `${project.globalSettings.ollamaBaseUrl}/api/chat`;
        headers = { 'Content-Type': 'application/json' };
        body.options = params;
    }

    const response = await fetchWithTimeout(url, {
        method: 'POST',
        headers: headers,
        body: JSON.stringify(body)
    });

    if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`API Error: ${errorText}`);
    }

    const data = await response.json();
    if (provider === 'openrouter' && data.choices && data.choices.length > 0) {
        return data.choices[0].message.content;
    }
    if (provider === 'ollama' && data.message) {
        return data.message.content;
    }
    
    throw new Error("Invalid API response structure.");
}

/**
 * [REWRITTEN] A more robust function to stream responses from LLM APIs.
 * It uses modern stream APIs to prevent data loss and parsing errors.
 */
export async function streamLLMResponse(agent, messages, onChunk) {
    const project = stateManager.getProject();
    const allModels = stateManager.getState().allProviderModels;
    const modelData = allModels.find(m => m.id === agent.model);
    if (!modelData) throw new Error("Model data not found for active agent.");

    const statusMessage = `Responding with ${modelData?.name || agent.model}...`;
    stateManager.bus.publish('status:update', { message: statusMessage, state: 'loading' });

    const provider = modelData.provider;
    let url, headers, body;

    // --- Setup request details based on provider ---
    if (provider === 'openrouter') {
        url = 'https://openrouter.ai/api/v1/chat/completions';
        headers = { 'Authorization': `Bearer ${project.globalSettings.apiKey}`, 'Content-Type': 'application/json' };
        body = { 
            model: agent.model, 
            messages, 
            stream: true,
            // [Feature] ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Web Search ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OpenRouter
            tools: [
                { "type": "Google Search" }
            ],
            ...agent.parameters
        };
    } else { // ollama
        // [Fix] ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏õ‡πá‡∏ô globalSettings (S ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏ç‡πà)
        url = `${project.globalSettings.ollamaBaseUrl}/api/chat`;
        headers = { 'Content-Type': 'application/json' };
        body = { 
            model: agent.model, 
            messages, 
            stream: true, 
            options: agent.parameters 
        };
    }

    try {
        const response = await fetchWithTimeout(url, {
            method: 'POST',
            headers,
            body: JSON.stringify(body),
            signal: stateManager.getState().abortController?.signal
        });

        if (!response.ok || !response.body) {
            throw new Error(`API Error: ${response.status} ${response.statusText}`);
        }

        // [Robustness] ‡πÉ‡∏ä‡πâ TextDecoderStream ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Stream ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        const reader = response.body.pipeThrough(new TextDecoderStream()).getReader();
        let fullResponseText = '';

        while (true) {
            const { value, done } = await reader.read();
            if (done) break;

            const rawChunks = value.split('\n').filter(line => line.trim() !== '');

            for (const chunk of rawChunks) {
                let token = '';
                try {
                    if (provider === 'openrouter') {
                        if (!chunk.startsWith('data: ')) continue;
                        const jsonStr = chunk.replace(/^data: /, '').trim();
                        if (jsonStr === '[DONE]') break;
                        
                        const data = JSON.parse(jsonStr);
                        token = data.choices?.[0]?.delta?.content || '';

                    } else { // ollama
                        const data = JSON.parse(chunk);
                        token = data.message?.content || '';
                    }
                } catch (e) {
                     console.warn("Skipping malformed JSON chunk during stream:", chunk);
                }

                if (token) {
                    fullResponseText += token;
                    onChunk(token); // ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å callback ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡πÉ‡∏´‡πâ UI
                }
            }
        }
        return fullResponseText; // ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏ö

    } catch (error) {
        if (error.name !== 'AbortError') {
             console.error("Streaming failed:", error);
             stateManager.bus.publish('status:update', { message: `Error: ${error.message}`, state: 'error' });
        }
        // ‡πÇ‡∏¢‡∏ô error ‡∏ï‡πà‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ (‡πÄ‡∏ä‡πà‡∏ô sendSingleAgentMessage) ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏õ
        throw error;
    }
}

export async function generateAndRenameSession(history){
     try{
        const project = stateManager.getProject();
        if(project.activeEntity.type !== 'agent') return;

        const agentName = project.activeEntity.name;
        const agent = project.agentPresets[agentName];
        if(!agent || !agent.model) return;
        
        const titlePrompt = `Based on the conversation, generate a concise title (3-5 words) and a single relevant emoji. Respond with a JSON object like {"title": "your title", "emoji": "üëç"}.`;
        
        const messages = [{ role: "user", content: titlePrompt }];
        const responseText = await callLLM({ ...agent, temperature: 0.2 }, messages);
        
        let newTitleData = {};
        try { newTitleData = JSON.parse(responseText.match(/{.*}/s)[0]); } 
        catch(e) { 
            console.error("Failed to parse title JSON:", responseText); 
            const titlePart = responseText.replace(/"/g, '').substring(0, 30);
            newTitleData = { title: titlePart, emoji: 'üí¨' };
        }
        
        const newTitle = `${newTitleData.emoji || 'üí¨'} ${newTitleData.title || 'Untitled'}`;

        if (newTitle) {
            stateManager.bus.publish('session:autoRename', { 
                sessionId: project.activeSessionId, 
                newName: newTitle 
            });
        }
    } catch(e) {
        console.error("Auto-rename failed:", e);
    }
}
